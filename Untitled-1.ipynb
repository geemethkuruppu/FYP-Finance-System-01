{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0e0ac2",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3673fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb1429",
   "metadata": {},
   "source": [
    "Step 1 â€“ Data Preprocessing: Clean and normalize transaction text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f9f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Transaction Description Merchant  \\\n",
      "15726                                        tea      NaN   \n",
      "15725                           Internet renewal      NaN   \n",
      "15724                    travels - Mumbai to brc      NaN   \n",
      "15723  Lunch - chicken fried rice + chicken soup      NaN   \n",
      "15722                                  3 bananas      NaN   \n",
      "\n",
      "                                  clean_desc       Date YearMonth  \n",
      "15726                                    tea 2015-01-13   2015-01  \n",
      "15725                       internet renewal 2015-01-13   2015-01  \n",
      "15724                  travels mumbai to brc 2015-01-14   2015-01  \n",
      "15723  lunch chicken fried rice chicken soup 2015-01-14   2015-01  \n",
      "15722                          <NUM> bananas 2015-01-14   2015-01  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"data/final_personal_finance_dataset.csv\")\n",
    "\n",
    "# Combine Merchant and Transaction Description \n",
    "if \"Merchant\" in df.columns:\n",
    "    df[\"raw_text\"] = (\n",
    "        df[\"Merchant\"].fillna(\"\").astype(str) + \" \" + \n",
    "        df[\"Transaction Description\"].fillna(\"\").astype(str)\n",
    "    )\n",
    "else:\n",
    "    df[\"raw_text\"] = df[\"Transaction Description\"].astype(str)\n",
    "\n",
    "# Clean text: lowercase, remove punctuation, collapse spaces, replace numbers\n",
    "df[\"clean_desc\"] = df[\"raw_text\"].str.lower()\n",
    "df[\"clean_desc\"] = df[\"clean_desc\"].str.replace(r\"[^a-z0-9 ]+\", \" \", regex=True)\n",
    "df[\"clean_desc\"] = df[\"clean_desc\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df[\"clean_desc\"] = df[\"clean_desc\"].str.replace(r\"\\b\\d+\\b\", \"<NUM>\", regex=True)\n",
    "\n",
    "# Convert date and add time-based features\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"Date\")\n",
    "df[\"YearMonth\"] = df[\"Date\"].dt.to_period(\"M\")\n",
    "\n",
    "print(df[[\"Transaction Description\", \"Merchant\", \"clean_desc\", \"Date\", \"YearMonth\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0107ee",
   "metadata": {},
   "source": [
    "Step 2 â€“ Feature Embeddings: Transform cleaned text into numerical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccdd7a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (15767, 12251)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. TF-IDF character n-gram features ---\n",
    "vectorizer = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), min_df=2)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"clean_desc\"])\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f231dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 493/493 [00:40<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT embedding shape: (15767, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. SBERT sentence embeddings ---\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  \n",
    "sbert_embeddings = model.encode(\n",
    "    df[\"clean_desc\"].tolist(),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "sbert_embeddings = np.array(sbert_embeddings)\n",
    "print(\"SBERT embedding shape:\", sbert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "463ce467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced TF-IDF shape: (15767, 200)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Combine both embeddings ---\n",
    "svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)  # directly from sparse\n",
    "print(\"Reduced TF-IDF shape:\", tfidf_reduced.shape)\n",
    "\n",
    "# Combine reduced TF-IDF with SBERT\n",
    "combined_features = np.hstack([tfidf_reduced, sbert_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ed19b",
   "metadata": {},
   "source": [
    "Step 3 â€“ Clustering (fast, POC-friendly): KMeans & HDBSCAN with SVD/PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f75d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sklearn version: 1.7.2\n",
      "[INFO] Clustering on 2000 rows (of 15767).\n",
      "[INFO] TF-IDF sample shape: (2000, 12251)\n",
      "[INFO] SBERT sample shape: (2000, 384)\n",
      "[INFO] Reducing TF-IDF with TruncatedSVD...\n",
      "    TF-IDF reduced shape: (2000, 200)\n",
      "    Combined feature shape: (2000, 584)\n",
      "[INFO] PCA to 50 dims...\n",
      "    Final feature shape: (2000, 50)\n",
      "[INFO] KMeans clustering (k=25, n_init=auto)...\n",
      "[INFO] Sample of labels: [24 24 22 24 22  1  1 22 22 22 22 22 22 24 24 24 11 24  6 24]\n",
      "[INFO] Clusters (excluding noise): 25\n",
      "[INFO] Noise points: 0\n",
      "[INFO] Exemplars (first 10): {0: 'mortgage payment mortgage payment purchase in mortgage rent', 1: 'auto <NUM> current residence to place <NUM>', 2: 'grocery store grocery store purchase in groceries', 3: 'credit card payment credit card payment purchase in paycheck', 4: 'biweekly paycheck biweekly paycheck purchase in groceries', 5: 'hardware store hardware store purchase in credit card payment', 6: 'internet service provider internet service provider purchase in groceries', 7: 'barbershop barbershop purchase in credit card payment', 8: 'gas company gas company purchase in credit card payment', 9: 'brewing company brewing company purchase in alcohol bars'}\n",
      "              Transaction Description  ClusterID\n",
      "15705                           broom         24\n",
      "15708             train - brc to mmct         24\n",
      "15703                          snacks         22\n",
      "15695                      chana chat         24\n",
      "15651                           bread         22\n",
      "15648  local - vadala road to Place 0          1\n",
      "15635               Place 0 to Bandra          1\n",
      "15630                             NaN         22\n",
      "15625                   chicken thali         22\n",
      "15624                             NaN         22\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & versions ---\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn\n",
    "try:\n",
    "    import hdbscan\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"Install hdbscan first: pip install hdbscan\") from e\n",
    "\n",
    "print(\"[INFO] sklearn version:\", sklearn.__version__)\n",
    "\n",
    "# --- Inputs expected from Step 2 ---\n",
    "# df                : DataFrame with df['clean_desc'] prepared\n",
    "# tfidf_matrix      : sparse TF-IDF matrix from Step 2\n",
    "# sbert_embeddings  : np.ndarray from Step 2 (n_samples, emb_dim)\n",
    "\n",
    "# Ensure SBERT embeddings are ndarray\n",
    "if not isinstance(sbert_embeddings, np.ndarray):\n",
    "    sbert_embeddings = np.asarray(sbert_embeddings)\n",
    "\n",
    "# 0) Runtime knobs\n",
    "USE_KMEANS = True               # True: KMeans (fast). False: HDBSCAN (better, slower)\n",
    "SAMPLE_N = 2000                 # Limit to N rows for clustering (set None to use all)\n",
    "N_SVD = 200                     # TruncatedSVD components for TF-IDF\n",
    "USE_PCA_AFTER_COMBINE = True    # Extra squeeze after combining features\n",
    "N_PCA = 50                      # PCA components for combined features\n",
    "K_FOR_KMEANS = 25               # Number of clusters for KMeans (tune per dataset)\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 5    # Typical starting point for HDBSCAN\n",
    "\n",
    "# 1) Sample for speed\n",
    "n = tfidf_matrix.shape[0]\n",
    "if SAMPLE_N is not None and SAMPLE_N < n:\n",
    "    rng = np.random.default_rng(42)\n",
    "    sample_idx = np.sort(rng.choice(n, SAMPLE_N, replace=False))\n",
    "else:\n",
    "    sample_idx = np.arange(n)\n",
    "\n",
    "df_sample = df.iloc[sample_idx].copy()\n",
    "tfidf_sample = tfidf_matrix[sample_idx]\n",
    "sbert_sample = sbert_embeddings[sample_idx]\n",
    "\n",
    "print(f\"[INFO] Clustering on {len(sample_idx)} rows (of {n}).\")\n",
    "print(\"[INFO] TF-IDF sample shape:\", tfidf_sample.shape)\n",
    "print(\"[INFO] SBERT sample shape:\", sbert_sample.shape)\n",
    "\n",
    "# 2) Reduce TF-IDF with TruncatedSVD (works on sparse)\n",
    "print(\"[INFO] Reducing TF-IDF with TruncatedSVD...\")\n",
    "svd = TruncatedSVD(n_components=min(N_SVD, tfidf_sample.shape[1]-1 if tfidf_sample.shape[1]>1 else 1),\n",
    "                   random_state=42)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_sample)\n",
    "print(\"    TF-IDF reduced shape:\", tfidf_reduced.shape)\n",
    "\n",
    "# 3) Combine reduced TF-IDF + SBERT\n",
    "if tfidf_reduced.shape[0] != sbert_sample.shape[0]:\n",
    "    raise ValueError(\"Row count mismatch between TF-IDF and SBERT after sampling.\")\n",
    "combined = np.hstack([tfidf_reduced, sbert_sample])\n",
    "print(\"    Combined feature shape:\", combined.shape)\n",
    "\n",
    "# 4) (Optional) PCA on combined\n",
    "if USE_PCA_AFTER_COMBINE:\n",
    "    n_pca = min(N_PCA, combined.shape[1], combined.shape[0])  # guard dims\n",
    "    print(f\"[INFO] PCA to {n_pca} dims...\")\n",
    "    pca = PCA(n_components=n_pca, random_state=42)\n",
    "    X = pca.fit_transform(combined)\n",
    "else:\n",
    "    X = combined\n",
    "print(\"    Final feature shape:\", X.shape)\n",
    "\n",
    "# 5) Clustering\n",
    "if USE_KMEANS:\n",
    "    # scikit-learn < 1.2 doesnâ€™t accept 'auto'\n",
    "    n_init_param = \"auto\" if tuple(map(int, sklearn.__version__.split(\".\")[:2])) >= (1,2) else 10\n",
    "    print(f\"[INFO] KMeans clustering (k={K_FOR_KMEANS}, n_init={n_init_param})...\")\n",
    "    kmeans = KMeans(n_clusters=K_FOR_KMEANS, n_init=n_init_param, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "else:\n",
    "    print(f\"[INFO] HDBSCAN clustering (min_cluster_size={HDBSCAN_MIN_CLUSTER_SIZE})...\")\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, metric=\"euclidean\")\n",
    "    labels = clusterer.fit_predict(X)\n",
    "\n",
    "# 6) Attach labels\n",
    "df_sample[\"ClusterID\"] = labels\n",
    "\n",
    "# 7) Exemplars (skip noise -1)\n",
    "non_noise = df_sample[\"ClusterID\"] != -1\n",
    "if non_noise.any():\n",
    "    exemplars = (\n",
    "        df_sample[non_noise]\n",
    "        .groupby(\"ClusterID\")[\"clean_desc\"]\n",
    "        .agg(lambda s: s.value_counts().index[0])\n",
    "        .to_dict()\n",
    "    )\n",
    "else:\n",
    "    exemplars = {}\n",
    "\n",
    "print(\"[INFO] Sample of labels:\", labels[:20])\n",
    "n_clusters = len(set(labels) - {-1})\n",
    "n_noise = int((labels == -1).sum()) if (-1 in labels) else 0\n",
    "print(f\"[INFO] Clusters (excluding noise): {n_clusters}\")\n",
    "print(f\"[INFO] Noise points: {n_noise}\")\n",
    "print(\"[INFO] Exemplars (first 10):\", dict(list(exemplars.items())[:10]))\n",
    "\n",
    "print(df_sample[[\"Transaction Description\", \"ClusterID\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8839f6",
   "metadata": {},
   "source": [
    "Step 4 â€“ Behavior feature engineering (per user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91f26a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Behavior feature sample:\n",
      "  UserID  micro_count_30d  payday_spike_ratio  subscription_count  \\\n",
      "0  US001                0            0.332194                  20   \n",
      "1  US002                0            0.185158                  40   \n",
      "2  US003                0            0.357886                  40   \n",
      "3  US004                0            0.236390                  41   \n",
      "4  US005                0            0.183549                  42   \n",
      "\n",
      "   subscription_monthly_spend  share_latest_Rent  share_latest_Shopping  \\\n",
      "0                    10938.92            0.70468               0.433470   \n",
      "1                        0.00            0.00000               0.000000   \n",
      "2                      138.86            0.00000               0.000000   \n",
      "3                      298.09            0.00000               0.501114   \n",
      "4                      407.39            0.00000               2.065164   \n",
      "\n",
      "   share_latest_Health & Fitness  share_latest_Salary  \\\n",
      "0                       0.218685             0.120115   \n",
      "1                       0.000000             0.000000   \n",
      "2                       0.000000             0.000000   \n",
      "3                       0.000000             0.000000   \n",
      "4                       0.000000             0.000000   \n",
      "\n",
      "   share_latest_Entertainment  ...  share_latest_healthcare  \\\n",
      "0                    0.111371  ...                      0.0   \n",
      "1                    0.000000  ...                      0.0   \n",
      "2                    0.000000  ...                      0.0   \n",
      "3                    0.000000  ...                      0.0   \n",
      "4                    0.000000  ...                      0.0   \n",
      "\n",
      "   share_latest_bills  share_latest_groceries  share_latest_entertainment  \\\n",
      "0                 0.0                     0.0                         0.0   \n",
      "1                 0.0                     0.0                         0.0   \n",
      "2                 0.0                     0.0                         0.0   \n",
      "3                 0.0                     0.0                         0.0   \n",
      "4                 0.0                     0.0                         0.0   \n",
      "\n",
      "   share_latest_transportation  share_latest_Money transfer  \\\n",
      "0                          0.0                          0.0   \n",
      "1                          0.0                          0.0   \n",
      "2                          0.0                          0.0   \n",
      "3                          0.0                          0.0   \n",
      "4                          0.0                          0.0   \n",
      "\n",
      "   share_latest_Investment  share_latest_Public Provident Fund  \\\n",
      "0                      0.0                                 0.0   \n",
      "1                      0.0                                 0.0   \n",
      "2                      0.0                                 0.0   \n",
      "3                      0.0                                 0.0   \n",
      "4                      0.0                                 0.0   \n",
      "\n",
      "   share_latest_subscription  share_latest_Other  \n",
      "0                        0.0                 0.0  \n",
      "1                        0.0                 0.0  \n",
      "2                        0.0                 0.0  \n",
      "3                        0.0                 0.0  \n",
      "4                        0.0                 0.0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assumes df has: UserID, Date (datetime), Amount (float), Type ('Income'/'Expense'),\n",
    "# Category (string), Merchant (string), YearMonth (Period['M'])\n",
    "\n",
    "def detect_subscriptions(g: pd.DataFrame) -> dict:\n",
    "    \"\"\"Return subscription merchants and monthly spend estimate.\"\"\"\n",
    "    exp = g[g['Type'] == 'Expense'].copy()\n",
    "    if exp.empty:\n",
    "        return {\"merchants\": [], \"monthly_spend\": 0.0}\n",
    "    # distinct months per merchant\n",
    "    months_per_merchant = exp.groupby('Merchant')['YearMonth'].nunique()\n",
    "    subs = months_per_merchant[months_per_merchant >= 3].index.tolist()\n",
    "    # spend in latest month (if available)\n",
    "    latest_month = g['YearMonth'].max()\n",
    "    monthly_spend = exp[(exp['Merchant'].isin(subs)) & (exp['YearMonth'] == latest_month)]['Amount'].sum()\n",
    "    return {\"merchants\": subs, \"monthly_spend\": float(monthly_spend)}\n",
    "\n",
    "def payday_spike_ratio(g: pd.DataFrame) -> float:\n",
    "    \"\"\"% of monthly expenses spent in 7 days after last income (salary heuristic).\"\"\"\n",
    "    income = g[g['Type'] == 'Income'].copy()\n",
    "    if income.empty:\n",
    "        return 0.0\n",
    "    # prefer salaries if labeled; else use largest income as proxy\n",
    "    salary_like = income[income['Category'].str.contains('salary', case=False, na=False)]\n",
    "    if salary_like.empty:\n",
    "        last_income_date = income.loc[income['Amount'].idxmax(), 'Date']\n",
    "    else:\n",
    "        last_income_date = salary_like['Date'].max()\n",
    "\n",
    "    exp = g[g['Type'] == 'Expense']\n",
    "    if exp.empty:\n",
    "        return 0.0\n",
    "\n",
    "    window_end = last_income_date + pd.Timedelta(days=7)\n",
    "    post_pay = exp[(exp['Date'] >= last_income_date) & (exp['Date'] <= window_end)]['Amount'].sum()\n",
    "\n",
    "    month_period = last_income_date.to_period('M')\n",
    "    total_month_exp = exp[exp['YearMonth'] == month_period]['Amount'].sum()\n",
    "    return float(post_pay / total_month_exp) if total_month_exp > 0 else 0.0\n",
    "\n",
    "def micro_spending_count(g: pd.DataFrame, threshold=10.0, days=30) -> int:\n",
    "    \"\"\"# of expense txns below threshold in the last N days.\"\"\"\n",
    "    if g.empty:\n",
    "        return 0\n",
    "    end = g['Date'].max()\n",
    "    start = end - pd.Timedelta(days=days)\n",
    "    mask = (g['Type'] == 'Expense') & (g['Date'].between(start, end)) & (g['Amount'] < threshold)\n",
    "    return int(mask.sum())\n",
    "\n",
    "def category_share_latest_month(g: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Expense share per category in latest month as fraction of income that month.\"\"\"\n",
    "    if g.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    latest = g['YearMonth'].max()\n",
    "    month = g[g['YearMonth'] == latest]\n",
    "    income_sum = month[month['Type'] == 'Income']['Amount'].sum()\n",
    "    exp = month[month['Type'] == 'Expense']\n",
    "    by_cat = exp.groupby('Category')['Amount'].sum()\n",
    "    if income_sum <= 0:\n",
    "        # fallback: share of total expenses\n",
    "        total_exp = by_cat.sum()\n",
    "        return (by_cat / total_exp) if total_exp > 0 else by_cat*0\n",
    "    return by_cat / income_sum\n",
    "\n",
    "# ---- Run per user and collect features ----\n",
    "features = []\n",
    "for uid, g in df.groupby('UserID'):\n",
    "    g = g.sort_values('Date')\n",
    "    feats = {\n",
    "        \"UserID\": uid,\n",
    "        \"micro_count_30d\": micro_spending_count(g, threshold=10.0, days=30),\n",
    "        \"payday_spike_ratio\": payday_spike_ratio(g),\n",
    "    }\n",
    "    subs = detect_subscriptions(g)\n",
    "    feats[\"subscription_count\"] = len(subs[\"merchants\"])\n",
    "    feats[\"subscription_monthly_spend\"] = subs[\"monthly_spend\"]\n",
    "\n",
    "    cat_share = category_share_latest_month(g)\n",
    "    # keep top 5 categories by share\n",
    "    for cat, val in cat_share.sort_values(ascending=False).head(5).items():\n",
    "        feats[f\"share_latest_{cat}\"] = float(val)\n",
    "\n",
    "    features.append(feats)\n",
    "\n",
    "feat_df = pd.DataFrame(features).fillna(0.0)\n",
    "print(\"[INFO] Behavior feature sample:\")\n",
    "print(feat_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6d175",
   "metadata": {},
   "source": [
    "Step 5 â€“ Pattern flags â†’ recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1852dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Recommendations preview:\n",
      "\n",
      "User US001:\n",
      " - ðŸ“º Review subscriptions: You have 20 active subscriptions (~LKR 10939/month). Consider canceling one you rarely use.\n",
      " - ðŸ“Š Rent is 70% of your income this month. Set a limit (e.g., 20%) and try a 1-week cutback challenge.\n",
      "\n",
      "User US002:\n",
      " - ðŸ“º Review subscriptions: You have 40 active subscriptions (~LKR 0/month). Consider canceling one you rarely use.\n",
      "\n",
      "User US003:\n",
      " - ðŸ“º Review subscriptions: You have 40 active subscriptions (~LKR 139/month). Consider canceling one you rarely use.\n",
      " - ðŸ“Š Gas & Fuel is 100% of your income this month. Set a limit (e.g., 20%) and try a 1-week cutback challenge.\n",
      "\n",
      "User US004:\n",
      " - ðŸ“º Review subscriptions: You have 41 active subscriptions (~LKR 298/month). Consider canceling one you rarely use.\n",
      " - ðŸ“Š Shopping is 50% of your income this month. Set a limit (e.g., 20%) and try a 1-week cutback challenge.\n",
      "\n",
      "User US005:\n",
      " - ðŸ“º Review subscriptions: You have 42 active subscriptions (~LKR 407/month). Consider canceling one you rarely use.\n",
      " - ðŸ“Š Shopping is 206% of your income this month. Set a limit (e.g., 20%) and try a 1-week cutback challenge.\n",
      "\n",
      "User US006:\n",
      " - ðŸ¦ Payday spike detected: 53% of your monthly spend happens in the week after payday. Try auto-saving 15% on payday and delay big buys by 7 days.\n",
      " - ðŸ“º Review subscriptions: You have 41 active subscriptions (~LKR 0/month). Consider canceling one you rarely use.\n",
      "\n",
      "User US008:\n",
      " - ðŸ“º Review subscriptions: You have 19 active subscriptions (~LKR 12042/month). Consider canceling one you rarely use.\n",
      " - ðŸ“Š healthcare is 40% of your income this month. Set a limit (e.g., 20%) and try a 1-week cutback challenge.\n",
      "\n",
      "User US009:\n",
      " - ðŸ¦ Payday spike detected: 100% of your monthly spend happens in the week after payday. Try auto-saving 15% on payday and delay big buys by 7 days.\n",
      " - ðŸ“º Review subscriptions: You have 59 active subscriptions (~LKR 420229/month). Consider canceling one you rarely use.\n",
      " - ðŸ“Š Money transfer is 145% of your income this month. Set a limit (e.g., 20%) and try a 1-week cutback challenge.\n"
     ]
    }
   ],
   "source": [
    "def make_flags(row) -> dict:\n",
    "    flags = {}\n",
    "    flags[\"micro_spender\"] = row[\"micro_count_30d\"] > 20                 # >20 small txns in last 30 days\n",
    "    flags[\"payday_spike\"] = row[\"payday_spike_ratio\"] > 0.5              # >50% of monthly spend in 7 days post-pay\n",
    "    flags[\"subs_heavy\"]   = row[\"subscription_count\"] > 5                 # >5 active subs\n",
    "    # overspending examples: look for any category share over 0.3 (30% of income)\n",
    "    flags[\"overspend_any\"] = any(v > 0.30 for k, v in row.items() if k.startswith(\"share_latest_\"))\n",
    "    return flags\n",
    "\n",
    "def make_recommendations(row) -> list:\n",
    "    recs = []\n",
    "    flags = make_flags(row)\n",
    "\n",
    "    if flags[\"micro_spender\"]:\n",
    "        recs.append(\n",
    "            f\"â˜• 3-day coffee challenge: You made {int(row['micro_count_30d'])} purchases under LKR 1000 in the last 30 days. \"\n",
    "            \"Try skipping cafÃ©/quick snacks for 3 days and track savings.\"\n",
    "        )\n",
    "    if flags[\"payday_spike\"]:\n",
    "        pct = int(row[\"payday_spike_ratio\"] * 100)\n",
    "        recs.append(\n",
    "            f\"ðŸ¦ Payday spike detected: {pct}% of your monthly spend happens in the week after payday. \"\n",
    "            \"Try auto-saving 15% on payday and delay big buys by 7 days.\"\n",
    "        )\n",
    "    if flags[\"subs_heavy\"]:\n",
    "        recs.append(\n",
    "            f\"ðŸ“º Review subscriptions: You have {int(row['subscription_count'])} active subscriptions \"\n",
    "            f\"(~LKR {row['subscription_monthly_spend']:.0f}/month). Consider canceling one you rarely use.\"\n",
    "        )\n",
    "    # overspend category (find the worst offender)\n",
    "    top_cat = max(((k.replace(\"share_latest_\",\"\"), v) for k,v in row.items() if k.startswith(\"share_latest_\")),\n",
    "                  key=lambda x: x[1], default=(None, 0))\n",
    "    if top_cat[0] and top_cat[1] > 0.30:\n",
    "        cat, share = top_cat\n",
    "        recs.append(\n",
    "            f\"ðŸ“Š {cat} is {int(share*100)}% of your income this month. \"\n",
    "            \"Set a limit (e.g., 20%) and try a 1-week cutback challenge.\"\n",
    "        )\n",
    "    return recs\n",
    "\n",
    "# Build final per-user recommendations\n",
    "out_rows = []\n",
    "for _, r in feat_df.iterrows():\n",
    "    recs = make_recommendations(r.to_dict())\n",
    "    out_rows.append({\"UserID\": r[\"UserID\"], \"recommendations\": recs})\n",
    "\n",
    "recs_df = pd.DataFrame(out_rows)\n",
    "print(\"\\n[INFO] Recommendations preview:\")\n",
    "for _, row in recs_df.iterrows():\n",
    "    print(f\"\\nUser {row['UserID']}:\")\n",
    "    for rec in row[\"recommendations\"]:\n",
    "        print(\" -\", rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f96195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
